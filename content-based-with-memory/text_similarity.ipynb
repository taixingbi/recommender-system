{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "googlenews model start loading...\n",
      "googlenews model load done\n",
      "\n",
      " init time taken:  0:02:06.825323 h:m:s \n",
      "\n",
      "This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--  vs  This is really nice apt\n",
      "score:  0.3043071 \n",
      "\n",
      "this apartment is great  vs  very nice apartment\n",
      "score:  0.8760053 \n",
      "\n",
      "this apartment is great  vs  bad house\n",
      "score:  0.5727214 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class pairDoc():\n",
    "    def __init__(self, path_pretained_model= 'data/GoogleNews-vectors-negative300.bin.gz'):\n",
    "        start_time= datetime.datetime.now()\n",
    "        \n",
    "        print(\"googlenews model start loading...\")\n",
    "        #load pretained google pretained model, this could take a few minutes\n",
    "        self.model_wv = gensim.models.KeyedVectors.load_word2vec_format( path_pretained_model, binary=True )\n",
    "        print(\"googlenews model load done\")\n",
    "        \n",
    "        delta= datetime.datetime.now()-start_time\n",
    "        print(\"\\nloading model time taken: \", delta, \"h:m:s \\n\")        \n",
    "        \n",
    "        #load stop words\n",
    "        self.stop_words= stopwords.words('english')\n",
    "\n",
    "    def doc_similarity(self, doc1, doc2):\n",
    "        def doc_vector(doc):\n",
    "            def tokenize_doc(doc):\n",
    "                #tokenize document\n",
    "                tokens= TweetTokenizer() .tokenize(doc)\n",
    "                #token sentense, remove number, stop word and lower case \n",
    "                token= [ token.lower() for token in tokens if token.isalpha() and token not in self.stop_words ] \n",
    "                return token\n",
    "\n",
    "            def wv2dv(words):#word vector to document vector\n",
    "                #if word not in vocab of google pretained model, then ignore it\n",
    "                words= [ word for word in words if word in self.model_wv.vocab ]\n",
    "                #average all words with 300 dimentional features\n",
    "                doc_vec = sum( self.model_wv[word] for word in words) / len(words)\n",
    "                return doc_vec\n",
    "\n",
    "            words= tokenize_doc(doc)\n",
    "            return wv2dv(words)\n",
    "\n",
    "        def cosine_sim(u, v):\n",
    "            #cosine similarity  cos sim= u * v* cos(u^v) \n",
    "            return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "        return cosine_sim( doc_vector(doc1), doc_vector(doc2) )        \n",
    "\n",
    "if __name__ == \"__main__\":   \n",
    "    path_pretained_model= 'pipenv/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "    p= pairDoc(path_pretained_model)\n",
    "\n",
    "    def pair_doc_similarity_score(pair):\n",
    "        print(pair[0], \" vs \", pair[1])\n",
    "        score= p.doc_similarity(pair[0], pair[1])\n",
    "        print( \"score: \", score, \"\\n\")\n",
    "    \n",
    "    #---------------------------test cases---------------------------\n",
    "    #test special char with number, stop word, and in vocab of google pretained model (for example, cooool)\n",
    "    pair = [\"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\",  \"This is really nice apt\"]    \n",
    "    pair_doc_similarity_score(pair)\n",
    "    \n",
    "    #test postive semantics, words' position changes \n",
    "    pair= [\"this apartment is great\", \"very nice apartment\"]\n",
    "    pair_doc_similarity_score(pair)\n",
    "    \n",
    "    #test nagative semantics\n",
    "    pair= [\"this apartment is great\", \"bad house\"]\n",
    "    pair_doc_similarity_score(pair)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
